{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from alu import ArithmeticAttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArithmeticAttentionModel()\n",
    "# model = torch.compile(model)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = torch.tensor([[1]]).float()\n",
    "num2 = torch.tensor([[2]]).float()\n",
    "op = torch.tensor([[0, 0, 1, 0]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13307.4150]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(num1, num2, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_loss(predictions, targets, scale_factor=10000.0):\n",
    "    abs_error = (predictions - targets)**2\n",
    "    # rel_error = torch.abs((predictions - targets) / (targets + 1e-8)) * scale_factor\n",
    "    loss = abs_error # + rel_error\n",
    "    return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "\n",
    "class ArithmeticDataset(IterableDataset):\n",
    "    def __init__(self, min_val=0, max_val=256):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        \n",
    "        self.operations = {\n",
    "            0: lambda x, y: x + y,    # addition\n",
    "            1: lambda x, y: x - y,    # subtraction\n",
    "            2: lambda x, y: x * y,    # multiplication\n",
    "            3: lambda x, y: x / (y + 1e-8)  # division\n",
    "        }\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # Generate random numbers\n",
    "            num1 = torch.rand(1) * (self.max_val - self.min_val) + self.min_val\n",
    "            num2 = torch.rand(1) * (self.max_val - self.min_val) + self.min_val\n",
    "            \n",
    "            # Generate random operations\n",
    "            op_idx = torch.tensor([0]) # torch.randint(0, 4, (1,))\n",
    "            operation = F.one_hot(op_idx, num_classes=4).float()\n",
    "            \n",
    "            # Calculate targets\n",
    "            target = self.operations[op_idx.item()](num1, num2)            \n",
    "            \n",
    "            yield num1, num2, operation.squeeze(0), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([49.4587]), tensor([241.3123]), tensor([1., 0., 0., 0.]), tensor([290.7710]))\n",
      "[tensor([[159.4502],\n",
      "        [ 26.7862]]), tensor([[ 14.7039],\n",
      "        [156.6797]]), tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]]), tensor([[174.1542],\n",
      "        [183.4658]])]\n"
     ]
    }
   ],
   "source": [
    "ad = ArithmeticDataset()\n",
    "print(next(iter(ad)))\n",
    "dataloader = torch.utils.data.DataLoader(ad, batch_size=2)\n",
    "print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (0.18.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (5.28.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hice1/avivekanand3/.conda/envs/py312/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mavivekanand\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "wandb.require(\"service\")\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    num_epochs=6000,\n",
    "    batch_size=1024,\n",
    "    initial_lr=1e-3,\n",
    "    device='cuda',\n",
    "    # eval_every=500,\n",
    "    use_wandb=False,\n",
    "    project_name=\"arithmetic_training\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.7)\n",
    "    \n",
    "    dataset = ArithmeticDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=24, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    steps_per_epoch = 1000\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Initialize logging\n",
    "    if use_wandb:\n",
    "        wandb.init(project=project_name)\n",
    "        wandb.config.update({\n",
    "            \"learning_rate\": initial_lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"scheduler_step_size\": 200,\n",
    "            \"scheduler_gamma\": 0.7\n",
    "        })\n",
    "    else:\n",
    "        # Create CSV log file with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = f'training_log_{timestamp}.csv'\n",
    "        log_data = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_diffs = []\n",
    "        \n",
    "        data_iter = iter(dataloader)\n",
    "        pbar = tqdm(range(steps_per_epoch), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for step in pbar:\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(dataloader)\n",
    "                batch = next(data_iter)\n",
    "            \n",
    "            num1, num2, operation, targets = [item.to(device) for item in batch]\n",
    "            \n",
    "            # num1 = num1.unsqueeze(1)\n",
    "            # num2 = num2.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(num1, num2, operation)\n",
    "            loss = arithmetic_loss(predictions, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                diffs = torch.abs(predictions - targets)\n",
    "                epoch_diffs.extend(diffs.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_num1, test_num2, test_op, test_targets = [item.to(device) for item in next(iter(dataloader))]\n",
    "            \n",
    "            test_pred = model(test_num1, test_num2, test_op)\n",
    "            test_loss = arithmetic_loss(test_pred, test_targets)\n",
    "           \n",
    "            first_pred = test_pred[0].item()\n",
    "            first_target = test_targets[0].item()\n",
    "            \n",
    "            # Format to 5 decimal places\n",
    "            first_pred_formatted = f\"{first_pred:.5f}\"\n",
    "            first_target_formatted = f\"{first_target:.5f}\"\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            train_loss = np.mean(epoch_losses)\n",
    "            val_loss = test_loss.item()\n",
    "            avg_diff = np.mean(epoch_diffs)\n",
    "            median_diff = np.median(epoch_diffs)\n",
    "            \n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'learning_rate': current_lr,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'avg_prediction_diff': avg_diff,\n",
    "                    'median_prediction_diff': median_diff,\n",
    "                    'epoch': epoch + 1\n",
    "                })\n",
    "            else:\n",
    "                log_data.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'learning_rate': current_lr,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'avg_prediction_diff': avg_diff,\n",
    "                    'median_prediction_diff': median_diff\n",
    "                })\n",
    "            \n",
    "            print(\n",
    "                f'Epoch {epoch+1}/{num_epochs} | '\n",
    "                f'LR: {current_lr:.2e} | '\n",
    "                f'Train Loss: {train_loss:.4f} | '\n",
    "                f'Val Loss: {val_loss:.4f} | '\n",
    "                f'Avg Diff: {avg_diff:.4f} | '\n",
    "                f'First Pred: {first_pred_formatted} | '\n",
    "                f'First Target: {first_target_formatted}'\n",
    "            )\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Save the best model\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), 'best_arithmetic_model.pt')\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1} completed. Average loss: {train_loss:.4f}\\n')\n",
    "    \n",
    "    if not use_wandb:\n",
    "        pd.DataFrame(log_data).to_csv(log_file, index=False)\n",
    "        print(f\"Training log saved to {log_file}\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1010/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.43it/s, Loss=400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1010/8000 | LR: 1.68e-05 | Train Loss: 166.2459 | Val Loss: 261.3722 | Avg Diff: 0.2923 | First Pred: 332.68600 | First Target: 333.09430\n",
      "Epoch 1010 completed. Average loss: 166.2459\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1011/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.39it/s, Loss=659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1011/8000 | LR: 1.68e-05 | Train Loss: 169.1221 | Val Loss: 240.1732 | Avg Diff: 0.2941 | First Pred: 386.65146 | First Target: 386.15509\n",
      "Epoch 1011 completed. Average loss: 169.1221\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1012/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.19it/s, Loss=396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1012/8000 | LR: 1.68e-05 | Train Loss: 424.1815 | Val Loss: 45.9742 | Avg Diff: 0.3056 | First Pred: 301.62262 | First Target: 301.29419\n",
      "Epoch 1012 completed. Average loss: 424.1815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1013/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.39it/s, Loss=20.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1013/8000 | LR: 1.68e-05 | Train Loss: 153.7865 | Val Loss: 48.5673 | Avg Diff: 0.2808 | First Pred: 259.82376 | First Target: 259.77542\n",
      "Epoch 1013 completed. Average loss: 153.7865\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1014/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 149.74it/s, Loss=480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1014/8000 | LR: 1.68e-05 | Train Loss: 156.9321 | Val Loss: 65.2211 | Avg Diff: 0.2811 | First Pred: 172.28333 | First Target: 172.23183\n",
      "Epoch 1014 completed. Average loss: 156.9321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1015/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.68it/s, Loss=149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1015/8000 | LR: 1.68e-05 | Train Loss: 171.2167 | Val Loss: 110.7710 | Avg Diff: 0.2987 | First Pred: 425.77570 | First Target: 426.39117\n",
      "Epoch 1015 completed. Average loss: 171.2167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1016/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.29it/s, Loss=353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1016/8000 | LR: 1.68e-05 | Train Loss: 166.5819 | Val Loss: 33.8810 | Avg Diff: 0.2895 | First Pred: 186.06664 | First Target: 185.98227\n",
      "Epoch 1016 completed. Average loss: 166.5819\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1017/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.41it/s, Loss=122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1017/8000 | LR: 1.68e-05 | Train Loss: 174.1205 | Val Loss: 13.5089 | Avg Diff: 0.3042 | First Pred: 130.03398 | First Target: 129.88109\n",
      "Epoch 1017 completed. Average loss: 174.1205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1018/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.32it/s, Loss=24.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1018/8000 | LR: 1.68e-05 | Train Loss: 198.2306 | Val Loss: 46.6368 | Avg Diff: 0.3136 | First Pred: 457.55304 | First Target: 457.67538\n",
      "Epoch 1018 completed. Average loss: 198.2306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1019/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.60it/s, Loss=53.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1019/8000 | LR: 1.68e-05 | Train Loss: 169.8156 | Val Loss: 81.1783 | Avg Diff: 0.2966 | First Pred: 187.59367 | First Target: 187.32611\n",
      "Epoch 1019 completed. Average loss: 169.8156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1020/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.22it/s, Loss=189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1020/8000 | LR: 1.68e-05 | Train Loss: 146.8020 | Val Loss: 28.5085 | Avg Diff: 0.2762 | First Pred: 108.51062 | First Target: 108.52539\n",
      "Epoch 1020 completed. Average loss: 146.8020\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1021/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.52it/s, Loss=152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1021/8000 | LR: 1.68e-05 | Train Loss: 176.0407 | Val Loss: 42.0107 | Avg Diff: 0.3049 | First Pred: 89.58910 | First Target: 89.70435\n",
      "Epoch 1021 completed. Average loss: 176.0407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1022/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 150.44it/s, Loss=42.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1022/8000 | LR: 1.68e-05 | Train Loss: 159.6724 | Val Loss: 117.4411 | Avg Diff: 0.2895 | First Pred: 319.42795 | First Target: 319.25626\n",
      "Epoch 1022 completed. Average loss: 159.6724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1023/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.42it/s, Loss=55.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1023/8000 | LR: 1.68e-05 | Train Loss: 165.4880 | Val Loss: 52.0345 | Avg Diff: 0.2895 | First Pred: 289.89569 | First Target: 290.15765\n",
      "Epoch 1023 completed. Average loss: 165.4880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1024/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.69it/s, Loss=174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1024/8000 | LR: 1.68e-05 | Train Loss: 170.5814 | Val Loss: 19.9781 | Avg Diff: 0.2982 | First Pred: 390.17258 | First Target: 390.13446\n",
      "Epoch 1024 completed. Average loss: 170.5814\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1025/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 151.89it/s, Loss=52.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1025/8000 | LR: 1.68e-05 | Train Loss: 171.0352 | Val Loss: 72.2521 | Avg Diff: 0.2991 | First Pred: 230.20943 | First Target: 230.07379\n",
      "Epoch 1025 completed. Average loss: 171.0352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1026/8000: 100%|███████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 151.71it/s, Loss=38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1026/8000 | LR: 1.68e-05 | Train Loss: 172.3733 | Val Loss: 53.0200 | Avg Diff: 0.2982 | First Pred: 277.15048 | First Target: 276.93744\n",
      "Epoch 1026 completed. Average loss: 172.3733\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1027/8000: 100%|███████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.10it/s, Loss=80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1027/8000 | LR: 1.68e-05 | Train Loss: 150.2276 | Val Loss: 117.6140 | Avg Diff: 0.2782 | First Pred: 296.11511 | First Target: 295.96985\n",
      "Epoch 1027 completed. Average loss: 150.2276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1028/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.67it/s, Loss=171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1028/8000 | LR: 1.68e-05 | Train Loss: 145.3700 | Val Loss: 92.3371 | Avg Diff: 0.2737 | First Pred: 254.25235 | First Target: 254.81607\n",
      "Epoch 1028 completed. Average loss: 145.3700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1029/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.20it/s, Loss=85.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1029/8000 | LR: 1.68e-05 | Train Loss: 176.8516 | Val Loss: 44.6769 | Avg Diff: 0.3001 | First Pred: 183.11528 | First Target: 183.08499\n",
      "Epoch 1029 completed. Average loss: 176.8516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1030/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.32it/s, Loss=27.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1030/8000 | LR: 1.68e-05 | Train Loss: 157.9176 | Val Loss: 80.4938 | Avg Diff: 0.2871 | First Pred: 94.08537 | First Target: 94.27412\n",
      "Epoch 1030 completed. Average loss: 157.9176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1031/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 151.97it/s, Loss=59.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1031/8000 | LR: 1.68e-05 | Train Loss: 171.7701 | Val Loss: 115.7973 | Avg Diff: 0.2994 | First Pred: 182.06241 | First Target: 181.49844\n",
      "Epoch 1031 completed. Average loss: 171.7701\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1032/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.06it/s, Loss=772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1032/8000 | LR: 1.68e-05 | Train Loss: 178.6181 | Val Loss: 122.0115 | Avg Diff: 0.3040 | First Pred: 281.16837 | First Target: 280.76016\n",
      "Epoch 1032 completed. Average loss: 178.6181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1033/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.50it/s, Loss=79.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1033/8000 | LR: 1.68e-05 | Train Loss: 159.5163 | Val Loss: 64.5733 | Avg Diff: 0.2895 | First Pred: 328.37570 | First Target: 327.98718\n",
      "Epoch 1033 completed. Average loss: 159.5163\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1034/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.89it/s, Loss=166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1034/8000 | LR: 1.68e-05 | Train Loss: 155.4957 | Val Loss: 19.6065 | Avg Diff: 0.2840 | First Pred: 449.58762 | First Target: 449.51794\n",
      "Epoch 1034 completed. Average loss: 155.4957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1035/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.44it/s, Loss=52.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1035/8000 | LR: 1.68e-05 | Train Loss: 169.9693 | Val Loss: 50.4017 | Avg Diff: 0.2933 | First Pred: 164.56195 | First Target: 164.48763\n",
      "Epoch 1035 completed. Average loss: 169.9693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1036/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 149.01it/s, Loss=495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1036/8000 | LR: 1.68e-05 | Train Loss: 164.9187 | Val Loss: 99.9524 | Avg Diff: 0.2937 | First Pred: 219.03949 | First Target: 219.32359\n",
      "Epoch 1036 completed. Average loss: 164.9187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1037/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.98it/s, Loss=95.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1037/8000 | LR: 1.68e-05 | Train Loss: 156.2741 | Val Loss: 513.7119 | Avg Diff: 0.2822 | First Pred: 362.50485 | First Target: 363.34015\n",
      "Epoch 1037 completed. Average loss: 156.2741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1038/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.34it/s, Loss=130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1038/8000 | LR: 1.68e-05 | Train Loss: 152.6377 | Val Loss: 173.0204 | Avg Diff: 0.2805 | First Pred: 197.47835 | First Target: 197.68036\n",
      "Epoch 1038 completed. Average loss: 152.6377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1039/8000: 100%|███████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.05it/s, Loss=56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1039/8000 | LR: 1.68e-05 | Train Loss: 166.9541 | Val Loss: 106.0646 | Avg Diff: 0.2914 | First Pred: 487.34338 | First Target: 487.66415\n",
      "Epoch 1039 completed. Average loss: 166.9541\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1040/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 150.87it/s, Loss=75.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1040/8000 | LR: 1.68e-05 | Train Loss: 171.1698 | Val Loss: 87.2581 | Avg Diff: 0.2936 | First Pred: 108.13628 | First Target: 108.00638\n",
      "Epoch 1040 completed. Average loss: 171.1698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1041/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.85it/s, Loss=39.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1041/8000 | LR: 1.68e-05 | Train Loss: 165.2986 | Val Loss: 60.6069 | Avg Diff: 0.2982 | First Pred: 306.07825 | First Target: 305.73480\n",
      "Epoch 1041 completed. Average loss: 165.2986\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1042/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 153.26it/s, Loss=47.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1042/8000 | LR: 1.68e-05 | Train Loss: 177.9516 | Val Loss: 73.0641 | Avg Diff: 0.3032 | First Pred: 229.44833 | First Target: 229.01724\n",
      "Epoch 1042 completed. Average loss: 177.9516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1043/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.55it/s, Loss=88.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1043/8000 | LR: 1.68e-05 | Train Loss: 162.3665 | Val Loss: 207.2420 | Avg Diff: 0.2912 | First Pred: 105.98170 | First Target: 106.03242\n",
      "Epoch 1043 completed. Average loss: 162.3665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1044/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.86it/s, Loss=211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1044/8000 | LR: 1.68e-05 | Train Loss: 168.6681 | Val Loss: 72.2936 | Avg Diff: 0.2956 | First Pred: 107.89833 | First Target: 107.86653\n",
      "Epoch 1044 completed. Average loss: 168.6681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1045/8000: 100%|██████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 151.41it/s, Loss=461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045/8000 | LR: 1.68e-05 | Train Loss: 145.4521 | Val Loss: 269.4271 | Avg Diff: 0.2754 | First Pred: 217.35196 | First Target: 216.80904\n",
      "Epoch 1045 completed. Average loss: 145.4521\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1046/8000: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.94it/s, Loss=14.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1046/8000 | LR: 1.68e-05 | Train Loss: 184.8154 | Val Loss: 448.9686 | Avg Diff: 0.3097 | First Pred: 169.06206 | First Target: 169.27998\n",
      "Epoch 1046 completed. Average loss: 184.8154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1047/8000:  60%|█████████████████████████████████████▊                         | 600/1000 [00:04<00:02, 153.94it/s, Loss=266]"
     ]
    }
   ],
   "source": [
    "train_model(model, num_epochs=8000, batch_size=1024, initial_lr=1e-4, device='cuda', use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
