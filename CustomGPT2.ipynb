{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4HLoBN3Swkc"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Model, GPT2Config\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
        "from typing import Optional, Tuple, Union # Import Optional, Tuple, and Union"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALU implementation"
      ],
      "metadata": {
        "id": "aH8GtPLBdVe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ALU(torch.nn.Module):\n",
        "    def __init__(self, model_dim=768, hidden_dim=512, internal_dim=10, use_output_projection=False):\n",
        "        super(ALU, self).__init__()\n",
        "\n",
        "        # input mlp does model_dim -> hidden_dim -> hidden_dim -> (internal_dim * 2 + 4)\n",
        "        self.input_mlp = nn.Sequential(\n",
        "            nn.Linear(model_dim, hidden_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_dim, internal_dim * 2 + 4),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        if use_output_projection:\n",
        "            # output projection does 1 -> internal_dim -> hidden_dim -> model_dim\n",
        "            self.output_projection = nn.Sequential(\n",
        "                nn.Linear(1, internal_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(internal_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, model_dim)\n",
        "            )\n",
        "\n",
        "        self.eps = 1e-8\n",
        "        self.base = torch.tensor([1, 2, 4, 8, 16, 32, 64, 128, 256, 512])\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"X-before: \", x.shape)\n",
        "        x = self.input_mlp(x)\n",
        "        a = x[:, :10]\n",
        "        b = x[:, 10:20]\n",
        "        op = x[:, 20:24]\n",
        "        print(\"X-after: \", x.shape)\n",
        "        print(\"A: \", a.shape)\n",
        "        print(\"B: \", b.shape)\n",
        "        print(\"OP: \", op.shape)\n",
        "        base = torch.tensor([1, 2, 4, 8, 16, 32, 64, 128, 256, 512], device=x.device, dtype=x.dtype)\n",
        "        a = torch.matmul(a, base)\n",
        "        b = torch.matmul(b, base)\n",
        "\n",
        "        op_weights = F.softmax(op, dim=1)  # Shape: (batch_size, 4)\n",
        "\n",
        "        add = a + b\n",
        "        sub = a - b\n",
        "        mul = a * b\n",
        "        div = a / (b + self.eps)\n",
        "\n",
        "        op_outs = torch.stack([add, sub, mul, div], dim=1)  # Shape: (batch_size, 4)\n",
        "        result = torch.sum(op_outs * op_weights, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
        "\n",
        "        if hasattr(self, 'output_projection'):\n",
        "            result = self.output_projection(result)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "f8Btv-rvdTqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard GPT-2"
      ],
      "metadata": {
        "id": "dBEfTankdbxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, GPT2Config\n",
        "configuration = GPT2Config()\n",
        "model = GPT2LMHeadModel(configuration)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNon2Ds5YkEl",
        "outputId": "e5293c84-f8e5-4eec-a5e9-84eb87d363ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2SdpaAttention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified GPT-2"
      ],
      "metadata": {
        "id": "6Q-QFXCkb2pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGPT2Block(GPT2Block):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.alu = ALU(model_dim=config.n_embd)\n",
        "        self.final_projection = nn.Linear(config.n_embd+24, config.n_embd)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
        "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "        attn_outputs = self.attn(\n",
        "            hidden_states,\n",
        "            layer_past=layer_past,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
        "        outputs = attn_outputs[1:]\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + residual\n",
        "\n",
        "        if encoder_hidden_states is not None:\n",
        "            # add one self-attention block for cross-attention\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
        "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "            residual = hidden_states\n",
        "            hidden_states = self.ln_cross_attn(hidden_states)\n",
        "            cross_attn_outputs = self.crossattention(\n",
        "                hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            attn_output = cross_attn_outputs[0]\n",
        "            # residual connection\n",
        "            hidden_states = residual + attn_output\n",
        "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
        "\n",
        "        alu_output = self.alu(hidden_states)     # NEW CODE: calling the ALU using the hidden_states\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_2(hidden_states)\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = residual + feed_forward_hidden_states\n",
        "        hidden_states = self.final_projection(torch.cat([hidden_states, alu_output], dim=-1))  # NEW CODE: concatenating the ALU output to the hidden states and projecting it to n_embd\n",
        "        outputs = (hidden_states,) + outputs\n",
        "\n",
        "        if use_cache:\n",
        "            outputs = (hidden_states,) + outputs\n",
        "        else:\n",
        "            outputs = (hidden_states,) + outputs[1:]\n",
        "\n",
        "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
      ],
      "metadata": {
        "id": "BDbO08rWb6Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGPT2Model(GPT2Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        num_layers = len(self.h)\n",
        "        for i in range(num_layers - 3, num_layers):\n",
        "            self.h[i] = CustomGPT2Block(config)\n",
        "\n",
        "    def forward(self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        return super().forward(input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)"
      ],
      "metadata": {
        "id": "Z_5hmrIkcBv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CustomGPT2Model(configuration)\n",
        "print(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y--X2HVdcJL7",
        "outputId": "bb2c576f-39ba-49d1-d526-e92583ecd6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomGPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-8): 9 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2SdpaAttention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (9-11): 3 x CustomGPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2SdpaAttention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (alu): ALU(\n",
            "        (input_mlp): Sequential(\n",
            "          (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "          (1): LeakyReLU(negative_slope=0.01)\n",
            "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (3): LeakyReLU(negative_slope=0.01)\n",
            "          (4): Linear(in_features=512, out_features=24, bias=True)\n",
            "          (5): LeakyReLU(negative_slope=0.01)\n",
            "        )\n",
            "      )\n",
            "      (final_projection): Linear(in_features=792, out_features=768, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later to load the weights"
      ],
      "metadata": {
        "id": "oYcll4KAcgcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config.from_pretrained('gpt2')\n",
        "customModel = CustomGPT2Model(config)\n",
        "\n",
        "# If you want to load pre-trained weights:\n",
        "state_dict = GPT2Model.from_pretrained('gpt2').state_dict()\n",
        "customModel.load_state_dict(state_dict, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "c1ea890bfe454d6dba8e5458d4c1c27f",
            "628615537dbf4af7be42d54434326c93",
            "83cd3a4674c846b2943873e830e85f35",
            "eb38db4b85014b6680c7732fedf109d9",
            "c5f6b051843347cfb6e8cfa044ba2d02",
            "66a6d32b299345fca9055b216f9c151e",
            "2be22d47a0ea47b9b7861e3c65af9977",
            "8693f5b1d359492e8d1677105c7f4e69",
            "da310b7895ad4871a199034d81b0839a",
            "9ca6812c8b0a487a9c643a6bbd229cb7",
            "6fe742f06e724a15909143f765ee4a38",
            "f84c37b166524cafa9a98a9f2492ccd4",
            "4d3c42fece3a41b3a3486e2748dd3162",
            "35a6f72e1b524711997c12a576c67f00",
            "4f26a000918c4d829dead4421b6aeffe",
            "36a416d59d274a5eaadb1914d9cae6c6",
            "f8b117c80bab404ebf25b025dd68b610",
            "bec07f69c1094d3f9da7ed9c17b56e29",
            "06924f2f9a114d0c85b4db82d8ee9411",
            "978c4ff377c0482ca88c75c08bb016a0",
            "7bac242118f649b2baa6b753e96eee70",
            "9a6110d741704346b359c03b566ab9d2"
          ]
        },
        "id": "MsEAWHjxcikD",
        "outputId": "4a7ed494-adcc-4adc-ae31-cc116ba6d74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1ea890bfe454d6dba8e5458d4c1c27f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f84c37b166524cafa9a98a9f2492ccd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['h.9.extra_linear.weight', 'h.9.extra_linear.bias', 'h.10.extra_linear.weight', 'h.10.extra_linear.bias', 'h.11.extra_linear.weight', 'h.11.extra_linear.bias'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}