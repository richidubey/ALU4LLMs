{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You have GPU access.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You have GPU access.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. You do not have GPU access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O4HLoBN3Swkc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rdubey36/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from typing import Optional, Tuple, Union # Import Optional, Tuple, and Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH8GtPLBdVe5"
   },
   "source": [
    "ALU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f8Btv-rvdTqc"
   },
   "outputs": [],
   "source": [
    "class ALU(torch.nn.Module):\n",
    "    def __init__(self, model_dim=768, hidden_dim=512, internal_dim=10, use_output_projection=False):\n",
    "        super(ALU, self).__init__()\n",
    "\n",
    "        # input mlp does model_dim -> hidden_dim -> hidden_dim -> (internal_dim * 2 + 4)\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(model_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, internal_dim * 2 + 4),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        if use_output_projection:\n",
    "            # output projection does 1 -> internal_dim -> hidden_dim -> model_dim\n",
    "            self.output_projection = nn.Sequential(\n",
    "                nn.Linear(1, internal_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(internal_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, model_dim)\n",
    "            )\n",
    "\n",
    "        self.eps = 1e-8\n",
    "        self.base = torch.tensor([1, 2, 4, 8, 16, 32, 64, 128, 256, 512])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"X-before: \", x.shape)\n",
    "        x = self.input_mlp(x)\n",
    "        a = x[:, :10]\n",
    "        b = x[:, 10:20]\n",
    "        op = x[:, 20:24]\n",
    "        # print(\"X-after: \", x.shape)\n",
    "        # print(\"A: \", a.shape)\n",
    "        # print(\"B: \", b.shape)\n",
    "        # print(\"OP: \", op.shape)\n",
    "        base = torch.tensor([1, 2, 4, 8, 16, 32, 64, 128, 256, 512], device=x.device, dtype=x.dtype)\n",
    "        a = torch.matmul(a, base)\n",
    "        b = torch.matmul(b, base)\n",
    "\n",
    "        op_weights = F.softmax(op, dim=1)  # Shape: (batch_size, 4)\n",
    "\n",
    "        add = a + b\n",
    "        sub = a - b\n",
    "        mul = a * b\n",
    "        div = a / (b + self.eps)\n",
    "\n",
    "        op_outs = torch.stack([add, sub, mul, div], dim=1)  # Shape: (batch_size, 4)\n",
    "        result = torch.sum(op_outs * op_weights, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        if hasattr(self, 'output_projection'):\n",
    "            result = self.output_projection(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBEfTankdbxj"
   },
   "source": [
    "Standard GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNon2Ds5YkEl",
    "outputId": "8f7a0522-a2b6-4d6b-9a78-aedc3b6823db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, GPT2Config\n",
    "configuration = GPT2Config()\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q-QFXCkb2pC"
   },
   "source": [
    "Modified GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BDbO08rWb6Gt"
   },
   "outputs": [],
   "source": [
    "class CustomGPT2Block(GPT2Block):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.alu = ALU(model_dim=config.n_embd, use_output_projection=True)\n",
    "        self.linear = nn.Linear(config.n_embd, config.n_embd)  # Linear\n",
    "        self.final_projection = nn.Linear(config.n_embd * 2, config.n_embd)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        alu_hidden_states = self.linear(hidden_states) # NEW CODE: using a linear layer to transform the current hidden_state for alu computation\n",
    "        summed_alu_hidden_states = alu_hidden_states.sum(dim=1)  # NEW CODE: summing across dimension 1 (sequence length) Shape: [batch_size, embedding_dim]\n",
    "        alu_output = self.alu(summed_alu_hidden_states)     # NEW CODE: calling the ALU using the hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "        hidden_states = torch.cat([hidden_states, alu_output.unsqueeze(1).expand(-1, hidden_states.size(1), -1)], dim=-1) # NEW CODE: concatenating the ALU output to the hidden states\n",
    "        hidden_states = self.final_projection(hidden_states)  # NEW CODE: projecting the hidden_state to the required dimension\n",
    "        outputs = (hidden_states,) + outputs\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Z_5hmrIkcBv0"
   },
   "outputs": [],
   "source": [
    "class CustomGPT2Model(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        num_layers = len(self.h)\n",
    "        for i in range(num_layers - 3, num_layers):\n",
    "            self.h[i] = CustomGPT2Block(config)\n",
    "        \n",
    "        #Add LM head\n",
    "        self.lm_head = torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        outputs =  super().forward(\n",
    "            input_ids,\n",
    "            past_key_values, \n",
    "            attention_mask, \n",
    "            token_type_ids, \n",
    "            position_ids, \n",
    "            head_mask, \n",
    "            inputs_embeds, \n",
    "            encoder_hidden_states, \n",
    "            encoder_attention_mask, \n",
    "            use_cache, \n",
    "            output_attentions, \n",
    "            output_hidden_states, \n",
    "            return_dict)\n",
    "    \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y--X2HVdcJL7",
    "outputId": "c1eecea2-a4c7-4a03-d66f-d49c99143f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomGPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-8): 9 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9-11): 3 x CustomGPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (alu): ALU(\n",
      "        (input_mlp): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): Linear(in_features=512, out_features=24, bias=True)\n",
      "          (5): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "        (output_projection): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=10, out_features=512, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=512, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (final_projection): Linear(in_features=1536, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = CustomGPT2Model(configuration)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYcll4KAcgcx"
   },
   "source": [
    "Later to load the weights (Need to verify this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379,
     "referenced_widgets": [
      "40d06beaf3184bab820b838f307eced0",
      "f1b7aa63a2b54b0da9fbb42cf150224f",
      "2e528a25513642a1bdc357462a93d6f3",
      "a661439821e644c489abdf913dde2f7f",
      "228a14a890c74f81a58ecf89759b2539",
      "ddf6e3191347473ab221282bc6befa06",
      "f1edee125e2145a0aaccdbcebcd5184e",
      "ca9af9908ea1428fb9e3a7463a53b78b",
      "87a5ba1d542b46668992e5d4f07909b7",
      "05f5aa7d478e4fc4a766718438cae2de",
      "1ea78e20a596498c834fb068380c2a44",
      "485bbc552f87497e80485c868face717",
      "d25b4beee8524b5db6c24d791165f9c4",
      "3dc783062d1d48339ce06229684cacba",
      "d91d67a1f0154660adbda90e3a4121ed",
      "5909802c97724dcebba8268a0a511677",
      "201b97b8813d41a4b7661d9f92e098ab",
      "527f56197cd649288b70c0f5d5510117",
      "b970dbe014d243299e909c5b954e02c4",
      "3799256f111445e8b19eb4c7da57f16d",
      "2c8b07c27f3242c8884aafe596e6a78d",
      "6db22affb0f74625bb174564a940a4e5"
     ]
    },
    "id": "MsEAWHjxcikD",
    "outputId": "8e510c21-3850-4f87-beac-85f6bb7af6e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['h.9.alu.input_mlp.0.weight', 'h.9.alu.input_mlp.0.bias', 'h.9.alu.input_mlp.2.weight', 'h.9.alu.input_mlp.2.bias', 'h.9.alu.input_mlp.4.weight', 'h.9.alu.input_mlp.4.bias', 'h.9.alu.output_projection.0.weight', 'h.9.alu.output_projection.0.bias', 'h.9.alu.output_projection.2.weight', 'h.9.alu.output_projection.2.bias', 'h.9.alu.output_projection.4.weight', 'h.9.alu.output_projection.4.bias', 'h.9.linear.weight', 'h.9.linear.bias', 'h.9.final_projection.weight', 'h.9.final_projection.bias', 'h.10.alu.input_mlp.0.weight', 'h.10.alu.input_mlp.0.bias', 'h.10.alu.input_mlp.2.weight', 'h.10.alu.input_mlp.2.bias', 'h.10.alu.input_mlp.4.weight', 'h.10.alu.input_mlp.4.bias', 'h.10.alu.output_projection.0.weight', 'h.10.alu.output_projection.0.bias', 'h.10.alu.output_projection.2.weight', 'h.10.alu.output_projection.2.bias', 'h.10.alu.output_projection.4.weight', 'h.10.alu.output_projection.4.bias', 'h.10.linear.weight', 'h.10.linear.bias', 'h.10.final_projection.weight', 'h.10.final_projection.bias', 'h.11.alu.input_mlp.0.weight', 'h.11.alu.input_mlp.0.bias', 'h.11.alu.input_mlp.2.weight', 'h.11.alu.input_mlp.2.bias', 'h.11.alu.input_mlp.4.weight', 'h.11.alu.input_mlp.4.bias', 'h.11.alu.output_projection.0.weight', 'h.11.alu.output_projection.0.bias', 'h.11.alu.output_projection.2.weight', 'h.11.alu.output_projection.2.bias', 'h.11.alu.output_projection.4.weight', 'h.11.alu.output_projection.4.bias', 'h.11.linear.weight', 'h.11.linear.bias', 'h.11.final_projection.weight', 'h.11.final_projection.bias', 'lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "customModel = CustomGPT2Model(config)\n",
    "\n",
    "# If you want to load pre-trained weights:\n",
    "state_dict = GPT2Model.from_pretrained('gpt2').state_dict()\n",
    "customModel.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-8): 9 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9-11): 3 x CustomGPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (alu): ALU(\n",
       "        (input_mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (3): LeakyReLU(negative_slope=0.01)\n",
       "          (4): Linear(in_features=512, out_features=24, bias=True)\n",
       "          (5): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (output_projection): Sequential(\n",
       "          (0): Linear(in_features=1, out_features=10, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=10, out_features=512, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=512, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (final_projection): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  antioxid antioxid antioxid antioxid antioxid\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "customModel.eval()\n",
    "input_text = \"Once upon a time,\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(customModel.device)\n",
    "#print(\"Device of input ids is\", input_ids.device)\n",
    "with torch.no_grad():\n",
    "    logits = customModel(input_ids=input_ids)\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "generated_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset to finetune\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "\n",
    "class ArithmeticDataset(IterableDataset):\n",
    "    def __init__(self, min_val=0, max_val=256):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        \n",
    "        self.operations = {\n",
    "            0: lambda x, y: x + y,    # addition\n",
    "            1: lambda x, y: x - y,    # subtraction\n",
    "            2: lambda x, y: x * y,    # multiplication\n",
    "            3: lambda x, y: x / (y + 1e-8)  # division\n",
    "        }\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # Generate random numbers\n",
    "            num1 = torch.rand(1) * (self.max_val - self.min_val) + self.min_val\n",
    "            num2 = torch.rand(1) * (self.max_val - self.min_val) + self.min_val\n",
    "            \n",
    "            # Generate random operations\n",
    "            op_idx = torch.tensor([0]) # torch.randint(0, 4, (1,))\n",
    "            operation = F.one_hot(op_idx, num_classes=4).float()\n",
    "            \n",
    "            # Calculate targets\n",
    "            target = self.operations[op_idx.item()](num1, num2)            \n",
    "            \n",
    "            yield num1, num2, operation.squeeze(0), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([74.8753]), tensor([202.4128]), tensor([1., 0., 0., 0.]), tensor([277.2881]))\n",
      "[tensor([[241.0134],\n",
      "        [176.9296]]), tensor([[139.6176],\n",
      "        [ 33.3208]]), tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]]), tensor([[380.6311],\n",
      "        [210.2504]])]\n"
     ]
    }
   ],
   "source": [
    "ad = ArithmeticDataset()\n",
    "print(next(iter(ad)))\n",
    "dataloader = torch.utils.data.DataLoader(ad, batch_size=2)\n",
    "print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (0.18.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hice1/rdubey36/.local/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hice1/rdubey36/.local/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hice1/rdubey36/.local/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /storage/ice1/1/6/rdubey36/.conda/envs/cv_proj4/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrichidubey\u001b[0m (\u001b[33mrichidubey-georgia-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "wandb.require(\"service\")\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "def arithmetic_loss(predictions, targets, scale_factor=10000.0):\n",
    "    abs_error = (predictions - targets)**2\n",
    "    # rel_error = torch.abs((predictions - targets) / (targets + 1e-8)) * scale_factor\n",
    "    loss = abs_error # + rel_error\n",
    "    return torch.sum(loss)\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    num_epochs=6000,\n",
    "    batch_size=1024,\n",
    "    initial_lr=1e-3,\n",
    "    device='cuda',\n",
    "    # eval_every=500,\n",
    "    use_wandb=False,\n",
    "    project_name=\"arithmetic_training\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.7)\n",
    "    \n",
    "    dataset = ArithmeticDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=24, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    steps_per_epoch = 1000\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Initialize logging\n",
    "    if use_wandb:\n",
    "        wandb.init(project=project_name)\n",
    "        wandb.config.update({\n",
    "            \"learning_rate\": initial_lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"scheduler_step_size\": 200,\n",
    "            \"scheduler_gamma\": 0.7\n",
    "        })\n",
    "    else:\n",
    "        # Create CSV log file with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = f'training_log_{timestamp}.csv'\n",
    "        log_data = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_diffs = []\n",
    "        \n",
    "        data_iter = iter(dataloader)\n",
    "        pbar = tqdm(range(steps_per_epoch), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for step in pbar:\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(dataloader)\n",
    "                batch = next(data_iter)\n",
    "            \n",
    "            num1, num2, operation, targets = [item.to(device) for item in batch]\n",
    "            \n",
    "            # num1 = num1.unsqueeze(1)\n",
    "            # num2 = num2.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inp_txt = str(num1) + str(operation) + str(num2)\n",
    "            input_ids = tokenizer(inp_txt, return_tensors=\"pt\")[\"input_ids\"].to(customModel.device)\n",
    "\n",
    "            logits = customModel(input_ids=input_ids)\n",
    "\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            predictions = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            try: \n",
    "                numeric_prediction = float(predictions)  # Only if it should be a number\n",
    "                predictions_tensor = torch.tensor([numeric_prediction]).to(device)  # Convert to tensor\n",
    "            except ValueError:\n",
    "                # print(f\"Decoded output is not numeric: {predictions}\")\n",
    "                predictions_tensor = torch.tensor([0.0], device=device, requires_grad=True)\n",
    "                \n",
    "            predictions =  predictions_tensor\n",
    "            loss = arithmetic_loss(predictions, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                diffs = torch.abs(predictions - targets)\n",
    "                epoch_diffs.extend(diffs.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_num1, test_num2, test_op, test_targets = [item.to(device) for item in next(iter(dataloader))]\n",
    "            \n",
    "            test_pred = model(test_num1, test_num2, test_op)\n",
    "            test_loss = arithmetic_loss(test_pred, test_targets)\n",
    "           \n",
    "            first_pred = test_pred[0].item()\n",
    "            first_target = test_targets[0].item()\n",
    "            \n",
    "            # Format to 5 decimal places\n",
    "            first_pred_formatted = f\"{first_pred:.5f}\"\n",
    "            first_target_formatted = f\"{first_target:.5f}\"\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            train_loss = np.mean(epoch_losses)\n",
    "            val_loss = test_loss.item()\n",
    "            avg_diff = np.mean(epoch_diffs)\n",
    "            median_diff = np.median(epoch_diffs)\n",
    "            \n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'learning_rate': current_lr,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'avg_prediction_diff': avg_diff,\n",
    "                    'median_prediction_diff': median_diff,\n",
    "                    'epoch': epoch + 1\n",
    "                })\n",
    "            else:\n",
    "                log_data.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'learning_rate': current_lr,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'avg_prediction_diff': avg_diff,\n",
    "                    'median_prediction_diff': median_diff\n",
    "                })\n",
    "            \n",
    "            print(\n",
    "                f'Epoch {epoch+1}/{num_epochs} | '\n",
    "                f'LR: {current_lr:.2e} | '\n",
    "                f'Train Loss: {train_loss:.4f} | '\n",
    "                f'Val Loss: {val_loss:.4f} | '\n",
    "                f'Avg Diff: {avg_diff:.4f} | '\n",
    "                f'First Pred: {first_pred_formatted} | '\n",
    "                f'First Target: {first_target_formatted}'\n",
    "            )\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Save the best model\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), 'best_arithmetic_model.pt')\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1} completed. Average loss: {train_loss:.4f}\\n')\n",
    "    \n",
    "    if not use_wandb:\n",
    "        pd.DataFrame(log_data).to_csv(log_file, index=False)\n",
    "        print(f\"Training log saved to {log_file}\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rdubey36/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrichidubey\u001b[0m (\u001b[33mrichidubey-georgia-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hice1/rdubey36/repos/ALU4LLMs/wandb/run-20241127_171218-8rt20ilg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/richidubey-georgia-institute-of-technology/arithmetic_training/runs/8rt20ilg' target=\"_blank\">fearless-darkness-5</a></strong> to <a href='https://wandb.ai/richidubey-georgia-institute-of-technology/arithmetic_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/richidubey-georgia-institute-of-technology/arithmetic_training' target=\"_blank\">https://wandb.ai/richidubey-georgia-institute-of-technology/arithmetic_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/richidubey-georgia-institute-of-technology/arithmetic_training/runs/8rt20ilg' target=\"_blank\">https://wandb.ai/richidubey-georgia-institute-of-technology/arithmetic_training/runs/8rt20ilg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rdubey36/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Epoch 1/8000:  80%|█████████████████████████████████████████████████████████████████████████████████████████████▉                       | 803/1000 [01:44<00:28,  6.91it/s, Loss=7.83e+7]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_model(customModel, num_epochs=8000, batch_size=1024, initial_lr=1e-4, device='cuda', use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cv_proj4]",
   "language": "python",
   "name": "conda-env-.conda-cv_proj4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05f5aa7d478e4fc4a766718438cae2de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ea78e20a596498c834fb068380c2a44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "201b97b8813d41a4b7661d9f92e098ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "228a14a890c74f81a58ecf89759b2539": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c8b07c27f3242c8884aafe596e6a78d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e528a25513642a1bdc357462a93d6f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca9af9908ea1428fb9e3a7463a53b78b",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87a5ba1d542b46668992e5d4f07909b7",
      "value": 665
     }
    },
    "3799256f111445e8b19eb4c7da57f16d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3dc783062d1d48339ce06229684cacba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b970dbe014d243299e909c5b954e02c4",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3799256f111445e8b19eb4c7da57f16d",
      "value": 548105171
     }
    },
    "40d06beaf3184bab820b838f307eced0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1b7aa63a2b54b0da9fbb42cf150224f",
       "IPY_MODEL_2e528a25513642a1bdc357462a93d6f3",
       "IPY_MODEL_a661439821e644c489abdf913dde2f7f"
      ],
      "layout": "IPY_MODEL_228a14a890c74f81a58ecf89759b2539"
     }
    },
    "485bbc552f87497e80485c868face717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d25b4beee8524b5db6c24d791165f9c4",
       "IPY_MODEL_3dc783062d1d48339ce06229684cacba",
       "IPY_MODEL_d91d67a1f0154660adbda90e3a4121ed"
      ],
      "layout": "IPY_MODEL_5909802c97724dcebba8268a0a511677"
     }
    },
    "527f56197cd649288b70c0f5d5510117": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5909802c97724dcebba8268a0a511677": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6db22affb0f74625bb174564a940a4e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87a5ba1d542b46668992e5d4f07909b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a661439821e644c489abdf913dde2f7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05f5aa7d478e4fc4a766718438cae2de",
      "placeholder": "​",
      "style": "IPY_MODEL_1ea78e20a596498c834fb068380c2a44",
      "value": " 665/665 [00:00&lt;00:00, 39.4kB/s]"
     }
    },
    "b970dbe014d243299e909c5b954e02c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca9af9908ea1428fb9e3a7463a53b78b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d25b4beee8524b5db6c24d791165f9c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_201b97b8813d41a4b7661d9f92e098ab",
      "placeholder": "​",
      "style": "IPY_MODEL_527f56197cd649288b70c0f5d5510117",
      "value": "model.safetensors: 100%"
     }
    },
    "d91d67a1f0154660adbda90e3a4121ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c8b07c27f3242c8884aafe596e6a78d",
      "placeholder": "​",
      "style": "IPY_MODEL_6db22affb0f74625bb174564a940a4e5",
      "value": " 548M/548M [00:02&lt;00:00, 241MB/s]"
     }
    },
    "ddf6e3191347473ab221282bc6befa06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1b7aa63a2b54b0da9fbb42cf150224f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddf6e3191347473ab221282bc6befa06",
      "placeholder": "​",
      "style": "IPY_MODEL_f1edee125e2145a0aaccdbcebcd5184e",
      "value": "config.json: 100%"
     }
    },
    "f1edee125e2145a0aaccdbcebcd5184e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
